{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "775d3566",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba32e3b3",
   "metadata": {},
   "source": [
    "This notebook includes code for preprocessing text-based journal data, focusing on dates and entries. It applies topic modeling using popular methods such as LDA, NMF, and LSA from the scikit-learn library. Additionally, sentiment analysis is performed using the VaderSentiment package. The resulting output file contains sentiment categories and scores, which will serve as input for the mental health analysis notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d4514e",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ccccb31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF, TruncatedSVD\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from transformers import pipeline\n",
    "# from bertopic import BERTopic\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94ce292",
   "metadata": {},
   "source": [
    "# Loading Data from a Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "833e81ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Text data is successfully uploaded!\n"
     ]
    }
   ],
   "source": [
    "# Function to load data from a .txt file\n",
    "def load_txt_file(file_path):\n",
    "    \"\"\"\n",
    "    Load text data from a .txt file.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): Path to the .txt file\n",
    "    \n",
    "    Returns:\n",
    "    str: Contents of the file as a single string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data = file.read()\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_path}' not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Specify the file path. Replace with personal journal text data.\n",
    "file_path = '2024_Journal.txt'  \n",
    "# Load the data\n",
    "text_data = load_txt_file(file_path)\n",
    "\n",
    "# Check the imported data\n",
    "if text_data:\n",
    "    print (\" Text data is successfully uploaded!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cab54d",
   "metadata": {},
   "source": [
    "# Checking for Available Dates in Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7492bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dates found: ['25th November', '26th November', '24th November', '27th November', 'Nov 28, 2024', 'Nov 30, 2024', 'Dec 1, 2024', 'Dec 2, 2024', 'Dec 3, 2024', 'Dec 4, 2024', 'Dec 5, 2024', 'Dec 6, 2024', 'Dec 7, 2024', 'Dec 8, 2024', 'Dec 9, 2024', 'Dec 10, 2024', 'Dec 11, 2024', 'Dec 12, 2024', 'Dec 13, 2024', 'Dec 14, 2024', 'Dec 15, 2024', 'Dec 16, 2024', 'Nov 24, 2024', 'Nov 21, 2024', 'Nov 18, 2024', 'Nov 12, 2024', '10th November', 'Nov 8, 2024', 'Nov 7, 2024', '16th September', 'Oct 8, 2024', 'Sep 30, 2024', 'Sep 28, 2024', 'Sep 27, 2024', 'Sep 18, 2024', 'Sep 17, 2024', 'Sep 16, 2024', 'Sep 9, 2024', 'Aug 9, 2024', 'Aug 8, 2024', '29th July', 'Jul 29, 2024', 'Jul 26, 2024', 'Jul 23, 2024', 'Jul 19, 2024', 'Jul 16, 2024', 'Jul 11, 2024', 'Jul 9, 2024', 'Jul 4, 2024', 'Jun 26, 2024', '24th August', 'Jun 18, 2024', 'May 29, 2024', 'May 21, 2024', 'May 10, 2024', 'May 1, 2024', 'Apr 19, 2024', 'Mar 26, 2024', 'Mar 19, 2024', 'Mar 15, 2024', 'Mar 13, 2024', 'Mar 12, 2024', 'Mar 9, 2024', 'Feb 28, 2024', 'Feb 21, 2024', 'Feb 15, 2024', 'Feb 13, 2024', 'Feb 12, 2024', 'Feb 9, 2024', 'Feb 8, 2024', 'Feb 6, 2024', 'Feb 1, 2024', 'Jan 30, 2024', 'Jan 26, 2024', 'Jan 17, 2024', 'Jan 11, 2024', 'Jan 9, 2024', 'Jan 3, 2024']\n",
      "Number of dates: 78\n"
     ]
    }
   ],
   "source": [
    "def extract_dates_from_file(file_path):\n",
    "    # Define a regular expression pattern to match dates in various formats used in the journal.\n",
    "    date_pattern = r\"\"\"\n",
    "        (\\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s\\d{1,2},\\s2024\\b)|  # Matches Jan 9, 2024\n",
    "        (\\b\\d{1,2}(?:st|nd|rd|th)?\\s(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\b)  # Matches 25th November\n",
    "    \"\"\"\n",
    "\n",
    "    # Compile the regular expression with multiline and ignore whitespace flags\n",
    "    date_regex = re.compile(date_pattern, re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:  # Specify UTF-8 encoding\n",
    "            content = file.read()\n",
    "\n",
    "        # Find all matches in the file content\n",
    "        matches = date_regex.findall(content)\n",
    "\n",
    "        # Flatten the matches and remove empty strings\n",
    "        dates = [date for group in matches for date in group if date]\n",
    "\n",
    "        return dates, len(dates)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"The file was not found. Please check the file path.\")\n",
    "        return [], 0\n",
    "    except UnicodeDecodeError as e:\n",
    "        print(f\"Unicode decode error: {e}. Try specifying the correct file encoding.\")\n",
    "        return [], 0\n",
    "\n",
    "# Example usage\n",
    "file_path = \"2024_Journal.txt\"  # Replace with your text file path\n",
    "dates, count = extract_dates_from_file(file_path)\n",
    "print(f\"Dates found: {dates}\")\n",
    "print(f\"Number of dates: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ca38fa",
   "metadata": {},
   "source": [
    "# Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e2183f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LDA Topics:\n",
      "Topic 1: stuff, like, really, chapter, days, work, going, need, life, time\n",
      "Topic 2: need, know, time, want, lord, god, feel, life, like, phone\n",
      "Topic 3: need, jul, nice, lord, far, future, thank, god, going, trend\n",
      "Topic 4: really, past, feel, god, days, like, going, time, life, need\n",
      "Topic 5: want, know, money, life, time, need, dec, went, like, day\n",
      "\n",
      "NMF Topics:\n",
      "Topic 1: stuff, think, position, phd, data, things, going, time, need, work\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bwire\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 2: believe, shall, live, pray, thinking, thank, chapter, lord, god, life\n",
      "Topic 3: school, interesting, going, today, enjoyed, hoped, super, far, jul, trend\n",
      "Topic 4: journal, lot, wasting, long, past, wasted, time, nov, month, days\n",
      "Topic 5: patience, know, let, gilbert, people, dont, lonely, need, like, feel\n",
      "\n",
      "LSA Topics:\n",
      "Topic 1: know, days, lord, feel, god, like, going, need, time, life\n",
      "Topic 2: far, pray, oh, live, thinking, shall, thank, god, lord, life\n",
      "Topic 3: future, thank, school, super, hoped, today, going, far, jul, trend\n",
      "Topic 4: screen, watching, th, wasting, wasted, cat, month, life, nov, days\n",
      "Topic 5: meeting, days, enjoyed, like, feel, jul, interesting, people, trend, super\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the text file\n",
    "def load_documents(file_path, delimiter='\\n\\n'):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        # Split the content into documents\n",
    "        documents = content.split(delimiter)\n",
    "        return [doc.strip() for doc in documents if doc.strip()]\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found. Check the file path.\")\n",
    "        return []\n",
    "\n",
    "def preprocess_text(documents):\n",
    "    stop_words = set(stopwords.words('english'))  # Load stop words\n",
    "    processed_docs = []\n",
    "    for doc in documents:\n",
    "        # Lowercase, remove special characters, and strip whitespace\n",
    "        doc = doc.lower()\n",
    "        doc = re.sub(r'[^a-z\\s]', '', doc)\n",
    "        doc = re.sub(r'\\s+', ' ', doc).strip()\n",
    "        \n",
    "        # Remove stop words\n",
    "        doc = ' '.join(word for word in doc.split() if word not in stop_words)\n",
    "        \n",
    "        processed_docs.append(doc)\n",
    "    return processed_docs\n",
    "\n",
    "# Topic Modeling using LDA\n",
    "def lda_topic_modeling(documents, n_topics=5):\n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "    dtm = vectorizer.fit_transform(documents)\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "    lda.fit(dtm)\n",
    "    topics = []\n",
    "    for idx, topic in enumerate(lda.components_):\n",
    "        top_words = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]]\n",
    "        topics.append(f\"Topic {idx + 1}: {', '.join(top_words)}\")\n",
    "    return topics\n",
    "\n",
    "# Topic Modeling using NMF\n",
    "def nmf_topic_modeling(documents, n_topics=5):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    dtm = vectorizer.fit_transform(documents)\n",
    "    nmf = NMF(n_components=n_topics, random_state=42)\n",
    "    nmf.fit(dtm)\n",
    "    topics = []\n",
    "    for idx, topic in enumerate(nmf.components_):\n",
    "        top_words = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]]\n",
    "        topics.append(f\"Topic {idx + 1}: {', '.join(top_words)}\")\n",
    "    return topics\n",
    "\n",
    "# Topic Modeling using LSA\n",
    "def lsa_topic_modeling(documents, n_topics=5):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    dtm = vectorizer.fit_transform(documents)\n",
    "    lsa = TruncatedSVD(n_components=n_topics, random_state=42)\n",
    "    lsa.fit(dtm)\n",
    "    topics = []\n",
    "    for idx, topic in enumerate(lsa.components_):\n",
    "        top_words = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]]\n",
    "        topics.append(f\"Topic {idx + 1}: {', '.join(top_words)}\")\n",
    "    return topics\n",
    "\n",
    "\n",
    "# Main function to test the techniques\n",
    "\n",
    "file_path = \"2024_Journal.txt\"  # Replace with your text file path\n",
    "delimiter = '\\n\\n'  # Define how documents are separated in your file\n",
    "\n",
    "# Load and preprocess documents\n",
    "documents = load_documents(file_path, delimiter)\n",
    "\n",
    "\n",
    "processed_docs = preprocess_text(documents)\n",
    "\n",
    "print(\"\\nLDA Topics:\")\n",
    "lda_topics = lda_topic_modeling(processed_docs)\n",
    "for topic in lda_topics:\n",
    "    print(topic)\n",
    "\n",
    "print(\"\\nNMF Topics:\")\n",
    "nmf_topics = nmf_topic_modeling(processed_docs)\n",
    "for topic in nmf_topics:\n",
    "    print(topic)\n",
    "\n",
    "print(\"\\nLSA Topics:\")\n",
    "lsa_topics = lsa_topic_modeling(processed_docs)\n",
    "for topic in lsa_topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985ac55d",
   "metadata": {},
   "source": [
    "# Put Data into a Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b74c1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract date and text from the file\n",
    "def extract_data_from_txt(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # Regular expression to match dates (e.g., Nov 25, 2024 or Dec 13, 2024)\n",
    "    date_pattern = r'(\\b[A-Za-z]{3,9} \\d{1,2}, \\d{4}\\b)'  # e.g., \"Nov 25, 2024\"\n",
    "    matches = re.split(date_pattern, content)\n",
    "\n",
    "    # Process matches to extract date and text pairs\n",
    "    data = []\n",
    "    for i in range(1, len(matches), 2):  # Start at index 1 for dates\n",
    "        date = matches[i].strip()\n",
    "        text = matches[i + 1].strip() if i + 1 < len(matches) else ''\n",
    "        data.append((date, text))\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data, columns=[\"Date\", \"Text\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "# Extract data and store in a DataFrame\n",
    "data_table = extract_data_from_txt(file_path)\n",
    "\n",
    "\n",
    "# Save the table to a CSV file\n",
    "data_table.to_csv('output_table.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4376ba9e",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ccfe5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = pd.read_csv('output_table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3783ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nov 28, 2024</td>\n",
       "      <td>I don’t remember what I did on this day. Wedne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nov 30, 2024</td>\n",
       "      <td>I went out to meet Bazil in the evening. I spe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dec 1, 2024</td>\n",
       "      <td>I did not go to church because of the rain. Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dec 2, 2024</td>\n",
       "      <td>I remember going to school to meet Glen. I bou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dec 3, 2024</td>\n",
       "      <td>Stayed at home the whole day, bought fried chi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Jan 26, 2024</td>\n",
       "      <td>Okay so we are coming to the end of January an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Jan 17, 2024</td>\n",
       "      <td>October 2023 to January 2024 Chat GPT Summary\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Jan 11, 2024</td>\n",
       "      <td>I am still procrastinating Howdy’s assignment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Jan 9, 2024</td>\n",
       "      <td>This morning, I had a major relapse. But it ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Jan 3, 2024</td>\n",
       "      <td>Today, I went to Tata Linda’s renewal of vows/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date                                               Text\n",
       "0   Nov 28, 2024  I don’t remember what I did on this day. Wedne...\n",
       "1   Nov 30, 2024  I went out to meet Bazil in the evening. I spe...\n",
       "2    Dec 1, 2024  I did not go to church because of the rain. Th...\n",
       "3    Dec 2, 2024  I remember going to school to meet Glen. I bou...\n",
       "4    Dec 3, 2024  Stayed at home the whole day, bought fried chi...\n",
       "..           ...                                                ...\n",
       "65  Jan 26, 2024  Okay so we are coming to the end of January an...\n",
       "66  Jan 17, 2024  October 2023 to January 2024 Chat GPT Summary\\...\n",
       "67  Jan 11, 2024  I am still procrastinating Howdy’s assignment ...\n",
       "68   Jan 9, 2024  This morning, I had a major relapse. But it ca...\n",
       "69   Jan 3, 2024  Today, I went to Tata Linda’s renewal of vows/...\n",
       "\n",
       "[70 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02aa195a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "def load_data(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "# Lexicon-Based Sentiment Analysis (VADER)\n",
    "def vader_sentiment_analysis(dataframe):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    sentiments = []\n",
    "    scores = []\n",
    "\n",
    "    for text in dataframe[\"Text\"]:\n",
    "        score = analyzer.polarity_scores(text)\n",
    "        sentiment = (\n",
    "            \"positive\" if score[\"compound\"] > 0.05\n",
    "            else \"negative\" if score[\"compound\"] < -0.05\n",
    "            else \"neutral\"\n",
    "        )\n",
    "        scores.append(score[\"compound\"])\n",
    "        sentiments.append(sentiment)\n",
    "    \n",
    "    dataframe[\"Sentiment Score\"] = scores\n",
    "    dataframe[\"Sentiment\"] = sentiments\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20c8bea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment analysis completed. Results saved to 'output_with_sentiments.csv'.\n"
     ]
    }
   ],
   "source": [
    "file_path = \"output_table.csv\"  # Path to the CSV file\n",
    "data = load_data(file_path)\n",
    "analyzed_data = vader_sentiment_analysis(data)\n",
    "    \n",
    "# Save the updated data to a new CSV file\n",
    "analyzed_data.to_csv(\"output_with_sentiments.csv\", index=False)\n",
    "print(\"Sentiment analysis completed. Results saved to 'output_with_sentiments.csv'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
